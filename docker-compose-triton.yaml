name: triton_server
services:
  triton:
    image: nvcr.io/nvidia/tritonserver:24.01-py3
    container_name: triton_server
    restart: unless-stopped
    ports:
      - "8000:8000" # HTTP endpoint
      - "8001:8001" # gRPC endpoint
      - "8002:8002" # Metrics endpoint
    volumes:
      - ./models:/models
    environment:
      - MODEL_ID=Qwen/Qwen2.5-VL-3B-Instruct
      - CHECKPOINT_PATH=/models/qwen_vl_fine_tuned/1/checkpoint-600
    command: >
      tritonserver --model-repository=/models
                   --log-verbose=1
                   --strict-model-config=false
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Add the client service from the oyjer file 
  client:
  image: nvcr.io/nvidia/tritonserver:24.01-py3-sdk  # Use pre-built image instead of build
  container_name: triton_client
  volumes:
    - ./:/workspace
  working_dir: /workspace
  environment:
    - TRITON_SERVER_URL=triton:8000
  command: >
    bash -c "pip install -q pandas matplotlib tqdm && tail -f /dev/null"
  depends_on:
    triton:
      condition: service_healthy